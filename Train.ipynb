{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4f39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ede93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15154361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r'D:\\Data\\Desktop\\ML ProjectDeaf and Dumb\\Dataset\\training_set', \n",
    "                                            target_size=(64,64), \n",
    "                                            batch_size=300, \n",
    "                                            class_mode='categorical',  \n",
    "                                            color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eafa4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'D': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7c1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "x_test = test_datagen.flow_from_directory(r'D:\\Data\\Desktop\\ML ProjectDeaf and Dumb\\Dataset\\test_set', \n",
    "                                          target_size=(64,64), \n",
    "                                          batch_size=300, \n",
    "                                          class_mode='categorical',  \n",
    "                                          color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5783d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e99a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(32, (3,3), input_shape=(64,64,1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66462b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0753f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3197b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(64, (3,3), input_shape=(64,64,1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab982ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65284dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acff789",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(128, (3,3), input_shape=(64,64,1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d54ff933",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21cd7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1e33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7fe6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=512, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4047434",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=9, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de9a7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(kernel_initializer='uniform',activation='softmax',units=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36650bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a18275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17/17 [==============================] - 18s 1s/step - loss: 0.1771 - accuracy: 0.9412 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.0604 - accuracy: 0.9832 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0316 - accuracy: 0.9909 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 3.7638e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 1.6470e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 5.6587e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 3.6609e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 8.8001e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.8931e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 20s 1s/step - loss: 4.1464e-04 - accuracy: 1.0000 - val_loss: 8.3524e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 9.1902e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = classifier.fit_generator(generator=x_train,\n",
    "                                   steps_per_epoch=5250//300,\n",
    "                                   epochs=15,\n",
    "                                   validation_data=x_test,\n",
    "                                   validation_steps=750//300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc82eca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f42de72940>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9062e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('gesture-copy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75e585eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = classifier.to_json()\n",
    "with open(\"model-bw-copy.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823ba70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
